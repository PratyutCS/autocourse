[
  {
    "filename": "1733303426New.pdf",
    "done": 1,
    "Program": "CoreElectiveâ€“6thSem,CSE",
    "Session": "3sessionsperweek",
    "course_code": "CSE3720",
    "course_name": "GenerativeAIandLLMs",
    "Module/Semester": "6th Semester",
    "course_description": "This course aims to provide students with a comprehensive understanding of generative artificial intelligence, focusing on large language models. Students will learn how to design, train, and deploy generative AI systems, with a particular emphasis on Large Language Models. They will explore the principles, applications, and ethical considerations surrounding these technologies. The course describes in detail the transformer architecture that powers LLMs, how they're trained, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.",
    "Course Syllabus": [
      {
        "srNo": 1,
        "content": "Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.",
        "co": "1",
        "sessions": 1
      },
      {
        "srNo": 2,
        "content": "Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational AutoencodersText-to-Text generation, Image and text-based VAE applications.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 3,
        "content": "Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 4,
        "content": "Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders. Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc. Gain deeper insights into the capabilities and potential of Transformer technology.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 5,
        "content": "LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text. Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs. Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs. Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 6,
        "content": "Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts. Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its components.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 7,
        "content": "Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and other ethical concerns. Addressing bias and fairness in generative AI systems Ensuring responsible use and deployment of generative models.",
        "co": "3",
        "sessions": 1
      }
    ],
    "Learning Resources": {
      "textBooks": [
        "Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play by David Foster, O'Reilly Media. Second edition: June 6, 2023 (ISBN: 978-1098134181)",
        "Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs by Ben Auffarth, Packt Publishing, first edition : December 22, 2023 (ISBN-13: 978-1835083468)"
      ],
      "referenceLinks": [
        "Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).",
        "Yenduri, Gokul, et al. \"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions.\" arXiv preprint arXiv:2305.10435 (2023)",
        "Liu, Yang, et al. \"Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.\" arXiv preprint arXiv:2308.05374 (2023)"
      ]
    },
    "internalAssessmentData": {
      "components": {
        "component1731776591362": {
          "component": "Assignment",
          "duration": "2 Weeks",
          "weightage": "20%",
          "evaluationWeek": "After Mid Assess",
          "remarks": "Continuous Participation + Viva"
        }
      }
    },
    "copoMappingData": {
      "courseOutcomes": {
        "CO1": {
          "description": "Develop a deep understanding of generative AI principles, encompassing various generative models and their practical applications for diverse real-world scenarios.",
          "bullets": []
        }
      },
      "mappingData": {
        "CO1": {
          "PO1": "3",
          "PO2": "3",
          "PO3": "3",
          "PO4": "2",
          "PO5": "3",
          "PO6": "3",
          "PO7": "3",
          "PO8": "3",
          "PO9": "3",
          "PO10": "0",
          "PO11": "0",
          "PO12": "0",
          "PSO1": "0",
          "PSO2": "0",
          "PSO3": "0",
          "PSO4": "0"
        }
      }
    },
    "actionsForWeakStudentsData": [
      {
        "id": "1",
        "text": ""
      }
    ],
    "mergePDF": "1733303482Xai (2).pdf",
    "course_syllabus": [
      {
        "srNo": 1,
        "content": "Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.",
        "co": "1",
        "sessions": 1
      },
      {
        "srNo": 2,
        "content": "Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational AutoencodersText-to-Text generation, Image and text-based VAE applications.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 3,
        "content": "Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 4,
        "content": "Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders. Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc. Gain deeper insights into the capabilities and potential of Transformer technology.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 5,
        "content": "LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text. Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs. Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs. Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 6,
        "content": "Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts. Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its components.",
        "co": "2",
        "sessions": 1
      },
      {
        "srNo": 7,
        "content": "Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and other ethical concerns. Addressing bias and fairness in generative AI systems Ensuring responsible use and deployment of generative models.",
        "co": "3",
        "sessions": 1
      }
    ],
    "studentListData": [],
    "weakStudentsData": [],
    "marksDetailsData": [],
    "attendanceReportData": []
  }
]