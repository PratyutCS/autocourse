[{"filename":"1732659376New.pdf","done":1,"Program":"CSE","Session":"6th Sem","course_code":"CSE3720","course_name":"Generative AI and LLMs","course_description":"This course aims to provide students with a comprehensive understanding of generative artificial intelligence, focusing on large language models. Students will learn how to design, train, and deploy generative AI systems, with a particular emphasis on Large Language Models. They will explore the principles, applications, and ethical considerations surrounding these technologies. The course describes in detail the transformer architecture that powers LLMs, how they’re trained, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","Course Syllabus":[{"srNo":1,"content":"Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.","co":"1","sessions":4},{"srNo":2,"content":"Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational Autoencoders Text-to-Text generation, Image and text-based VAE applications.","co":"2","sessions":3},{"srNo":3,"content":"Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.","co":"2","sessions":2},{"srNo":4,"content":"Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders, Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc, Gain deeper insights into the capabilities and potential of Transformer technology.","co":"2","sessions":5},{"srNo":5,"content":"LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text, Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs, Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs, Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","co":"2","sessions":6},{"srNo":6,"content":"Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts, Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its component.","co":"2","sessions":5},{"srNo":7,"content":"Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and Other ethical concerns Addressing bias and fairness in generative AI systems Ensuring responsible use and deployment of generative models.","co":"3","sessions":5}],"Learning Resources":{"textBooks":["Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play by David Foster, O'Reilly Media, Second edition: June 6, 2023 (ISBN: 978-1098134181)","Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs by Ben Auffarth, Packt Publishing, first edition: December 22, 2023 (ISBN-13: 978-1835083468)"],"referenceLinks":["Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems 30 (2017).","Yenduri, Gokul, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Rutvij H. Jhaveri, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy Gadekallu. Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. arXiv preprint arXiv:2305.10435 (2023).","Liu, Yang, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. arXiv preprint arXiv:2308.05374 (2023)."]},"internalAssessmentData":{"components":{"Assignment":{"component":"Assignment","duration":"2 Weeks","weightage":"20%","evaluationWeek":"","remarks":"Continuous Participation + Viva"},"Quiz":{"component":"Quiz","duration":"20 mins","weightage":"20%","evaluationWeek":"","remarks":"Continuous"},"Project proposal":{"component":"Project proposal","duration":"","weightage":"40%","evaluationWeek":"","remarks":"Project-Based"},"End Term Project":{"component":"End Term Project","duration":"","weightage":"40%","evaluationWeek":"","remarks":"Project-Based"}}},"copoMappingData":{"courseOutcomes":{"CO1":{"description":"Develop a deep understanding of generative AI principles, encompassing various generative models and their practical applications for diverse real-world scenarios.","bullets":[]},"CO2":{"description":"Apply Generative Adversarial Networks (GANs), LLMs, and Sequence Models specifically Seq2Seq models and Variational Autoencoders (VA2Es), strategies for designing effective prompts.","bullets":[]},"CO3":{"description":"Evaluate frameworks that ensure the responsible deployment of generative models, fostering a commitment to ethical considerations.","bullets":[]}},"mappingData":{},"tableMode":"image","imagePath":"/images/1732659407582-WhatsApp Image 2024-11-02 at 07.59.31.jpeg"},"actionsForWeakStudentsData":[{"id":"1","text":""}],"Module/Semester":"","course_syllabus":[{"srNo":1,"content":"Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.","co":"1","sessions":4},{"srNo":2,"content":"Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational Autoencoders Text-to-Text generation, Image and text-based VAE applications.","co":"2","sessions":3},{"srNo":3,"content":"Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.","co":"2","sessions":2},{"srNo":4,"content":"Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders, Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc, Gain deeper insights into the capabilities and potential of Transformer technology.","co":"2","sessions":5},{"srNo":5,"content":"LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text, Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs, Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs, Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","co":"2","sessions":6},{"srNo":6,"content":"Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts, Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its component.","co":"2","sessions":5},{"srNo":7,"content":"Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and Other ethical concerns Addressing bias and fairness in generative AI systems Ensuring responsible use and deployment of generative models.","co":"3","sessions":5}],"studentListData":[],"weakStudentsData":[],"marksDetailsData":[],"attendanceReportData":[]},{"filename":"1732660930New.pdf","done":1,"Program":"CSE","Session":"6th Sem","course_code":"CSE3720","course_name":"Generative AI and LLMs","course_description":"This course aims to provide students with a comprehensive understanding of generative artificial intelligence, focusing on large language models. Students will learn how to design, train, and deploy generative AI systems, with a particular emphasis on Large Language Models. They will explore the principles, applications, and ethical considerations surrounding these technologies. The course describes in detail the transformer architecture that powers LLMs, how they’re trained, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","Course Syllabus":[{"srNo":1,"content":"Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.","co":"1","sessions":4},{"srNo":2,"content":"Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational Autoencoders Text-to-Text generation, Image and text-based VAE applications.","co":"2","sessions":3},{"srNo":3,"content":"Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.","co":"2","sessions":2},{"srNo":4,"content":"Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders, Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc, Gain deeper insights into the capabilities and potential of Transformer technology.","co":"2","sessions":5},{"srNo":5,"content":"LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text, Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs, Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs, Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","co":"2","sessions":6},{"srNo":6,"content":"Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts, Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its component.","co":"2","sessions":5},{"srNo":7,"content":"Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and Other ethical concerns Addressing bias and fairness in generative AI systems Ensuring responsible use and deployment of generative models.","co":"3","sessions":5}],"Learning Resources":{"textBooks":["Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play by David Foster, O'Reilly Media, Second edition: June 6, 2023 (ISBN: 978-1098134181)","Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs by Ben Auffarth, Packt Publishing, first edition: December 22, 2023 (ISBN-13: 978-1835083468)"],"referenceLinks":["Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems 30 (2017).","Yenduri, Gokul, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Rutvij H. Jhaveri, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy Gadekallu. Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. arXiv preprint arXiv:2305.10435 (2023).","Liu, Yang, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. arXiv preprint arXiv:2308.05374 (2023)."]},"internalAssessmentData":{"components":{"Assignment":{"component":"Assignment","duration":"2 Weeks","weightage":"20%","evaluationWeek":"","remarks":"Continuous Participation + Viva"},"Quiz":{"component":"Quiz","duration":"20 mins","weightage":"20%","evaluationWeek":"","remarks":"Continuous"},"Project proposal":{"component":"Project proposal","duration":"","weightage":"40%","evaluationWeek":"","remarks":"Project-Based"},"End Term Project":{"component":"End Term Project","duration":"","weightage":"40%","evaluationWeek":"","remarks":"Project-Based"}}},"copoMappingData":{"courseOutcomes":{"CO1":{"description":"Develop a deep understanding of generative AI principles, encompassing various generative models and their practical applications for diverse real-world scenarios.","bullets":[]},"CO2":{"description":"Apply Generative Adversarial Networks (GANs), LLMs, and Sequence Models specifically Seq2Seq models and Variational Autoencoders (VA2Es), strategies for designing effective prompts.","bullets":[]},"CO3":{"description":"Evaluate frameworks that ensure the responsible deployment of generative models, fostering a commitment to ethical considerations.","bullets":[]}},"mappingData":{"CO1":{"PO1":"3","PO2":"3","PO3":"3","PO4":"2","PO5":"3","PO6":"3","PO7":"3","PO8":"3","PO9":"3","PO10":"3","PO11":"3","PO12":"3","PSO1":"0","PSO2":"0","PSO3":"0","PSO4":"0"},"CO2":{"PO1":"2","PO2":"3","PO3":"3","PO4":"3","PO5":"2","PO6":"1","PO7":"3","PO8":"3","PO9":"3","PO10":"3","PO11":"3","PO12":"3","PSO1":"0","PSO2":"0","PSO3":"0","PSO4":"0"},"CO3":{"PO1":"2","PO2":"3","PO3":"3","PO4":"2","PO5":"3","PO6":"3","PO7":"1","PO8":"3","PO9":"3","PO10":"3","PO11":"3","PO12":"3","PSO1":"1","PSO2":"1","PSO3":"1","PSO4":"1"}}},"actionsForWeakStudentsData":[{"id":"1","text":""}],"Module/Semester":"","course_syllabus":[{"srNo":1,"content":"Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.","co":"1","sessions":4},{"srNo":2,"content":"Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational Autoencoders Text-to-Text generation, Image and text-based VAE applications.","co":"2","sessions":3},{"srNo":3,"content":"Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.","co":"2","sessions":2},{"srNo":4,"content":"Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders, Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc, Gain deeper insights into the capabilities and potential of Transformer technology.","co":"2","sessions":5},{"srNo":5,"content":"LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text, Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs, Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs, Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","co":"2","sessions":6},{"srNo":6,"content":"Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts, Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its component.","co":"2","sessions":5},{"srNo":7,"content":"Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and Other ethical concerns Addressing bias and fairness in generative AI systems Ensuring responsible use and deployment of generative models.","co":"3","sessions":5}],"studentListData":[{"uniqueId":"210C2030002","studentName":"Aayush Dubey"},{"uniqueId":"210C2030004","studentName":"PelletiSujith Reddy"},{"uniqueId":"210C2030007","studentName":"Malladi Sai Prabhas"},{"uniqueId":"210C2030010","studentName":"SubhranshBehura"},{"uniqueId":"210C2030014","studentName":"Abhimanyu Gulati"}],"weakStudentsData":[{"uniqueId":"210C2030004","studentName":"PelletiSujith Reddy","totalMarks":78.5,"grade":"B"}],"marksDetailsData":[{"uniqueId":"210C2030002","studentName":"Aayush Dubey","totalMarks":83,"grade":"A+"},{"uniqueId":"210C2030004","studentName":"PelletiSujith Reddy","totalMarks":78.5,"grade":"B"},{"uniqueId":"210C2030007","studentName":"Malladi Sai Prabhas","totalMarks":81.5,"grade":"B+"},{"uniqueId":"210C2030010","studentName":"SubhranshBehura","totalMarks":87,"grade":"A+"},{"uniqueId":"210C2030014","studentName":"Abhimanyu Gulati","totalMarks":47,"grade":"R"}],"attendanceReportData":[{"uniqueId":"210C2030002","studentName":"Aayush Dubey","attendance":88},{"uniqueId":"210C2030004","studentName":"PelletiSujith Reddy","attendance":90},{"uniqueId":"210C2030007","studentName":"Malladi Sai Prabhas","attendance":78},{"uniqueId":"210C2030010","studentName":"SubhranshBehura","attendance":82},{"uniqueId":"210C2030014","studentName":"Abhimanyu Gulati","attendance":68}]}]