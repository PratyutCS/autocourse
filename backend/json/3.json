[{"filename":"1733315412New.pdf","done":1,"Program":"CoreElective–6thSem,CSE","Session":"3sessionsperweek [Eachsessionisof55minutes]","course_code":"CSE3720","course_name":"GenerativeAIandLLMs","Module/Semester":"6th Semester","course_description":"This course aims to provide students with a comprehensive understanding of generative artificial intelligence, focusing on large language models. Students will learn how to design, train, and deploy generative AI systems, with a particular emphasis on Large Language Models. They will explore the principles, applications, and ethical considerations surrounding these technologies. The course describes in detail the transformer architecture that powers LLMs, how they’re trained, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","Course Syllabus":{"courseSyllabus":[{"srNo":1,"content":"Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.","co":"1","sessions":1},{"srNo":2,"content":"Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational Autoencoders Text-to-Text generation, Image and text-based VAE applications.","co":"2","sessions":1},{"srNo":3,"content":"Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.","co":"2","sessions":1},{"srNo":4,"content":"Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders. Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc. Gain deeper insights into the capabilities and potential of Transformer technology.","co":"2","sessions":1},{"srNo":5,"content":"LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text. Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs. Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs. Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","co":"2","sessions":1},{"srNo":6,"content":"Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts. Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its components.","co":"2","sessions":1},{"srNo":7,"content":"Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and Other ethical concerns Addressing bias and fairness in generative AI systems Ensuring responsible use and deployment of generative models.","co":"3","sessions":1}]},"Learning Resources":{"textBooks":["Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play by David Foster, O'Reilly Media. Second edition: June 6, 2023 (ISBN:978-1098134181)","Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs by Ben Auffarth, Packt Publishing, first edition : December 22, 2023 (ISBN-13: 978-1835083468)"],"referenceLinks":["Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017). [Research Paper]","Yenduri, Gokul, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Rutvij H. Jhaveri, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy Gadekallu. \"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions.\" arXiv preprint arXiv:2305.10435 (2023). [Research Paper]","Liu, Yang, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. \"Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.\" arXiv preprint arXiv:2308.05374 (2023). [Research Paper]"]},"internalAssessmentData":{"components":{"component1731776591362":{"component":"Project as mentioned in the above table is the experiential learning component for this course. Students will be given challenging real-life problems. They will be asked to build solutions by applying suitable learning algorithms. The students are also expected to implement and show results of the proposed solution and perform a comparative analysis with different available algorithms. A separate assessment will be conducted for evaluating the solution provided by each student. The students are also expected to implement and show the results of the proposed solution or attempt to reimplement and improve on a research paper on a topic of their choice. Approximately 60-70% is experiential learning.","duration":"","weightage":"","evaluationWeek":"","remarks":""}}},"copoMappingData":{"courseOutcomes":{"CO1":{"description":"Develop a deep understanding of generative AI principles, encompassing various generative models and their practical applications for diverse real-world scenarios.","bullets":[]}},"mappingData":{"CO1":{"PO1":"3","PO2":"3","PO3":"3","PO4":"2","PO5":"3","PO6":"3","PO7":"3","PO8":"3","PO9":"0","PO10":"0","PO11":"0","PO12":"0","PSO1":"0","PSO2":"0","PSO3":"0","PSO4":"0"},"CO2":{"PO1":"2","PO2":"3","PO3":"3","PO4":"3","PO5":"2","PO6":"1","PO7":"3","PO8":"3","PO9":"0","PO10":"0","PO11":"0","PO12":"0","PSO1":"0","PSO2":"0","PSO3":"0","PSO4":"0"},"CO3":{"PO1":"2","PO2":"3","PO3":"3","PO4":"2","PO5":"3","PO6":"3","PO7":"1","PO8":"3","PO9":"3","PO10":"3","PO11":"1","PO12":"1","PSO1":"0","PSO2":"0","PSO3":"0","PSO4":"0"}}},"actionsForWeakStudentsData":[{"id":"1","text":""}]},{"filename":"1733315565New.pdf","done":1,"Program":"Core Elective-6th Sem, CSE","Session":"3 sessions per week","course_code":"CSE3720","course_name":"Generative AI and LLMs","Module/Semester":"6th Semester","course_description":"This course aims to provide students with a comprehensive understanding of generative artificial intelligence, focusing on large language models. Students will learn how to design, train, and deploy generative AI systems, with a particular emphasis on Large Language Models. They will explore the principles, applications, and ethical considerations surrounding these technologies. The course describes in detail the transformer architecture that powers LLMs, how they’re trained, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","Course Syllabus":[{"srNo":1,"content":"Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case, Generative AI Project Lifecycle","co":"1","sessions":1},{"srNo":2,"content":"Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational Autoencoders Text-to-Text generation, Image and text-based VAE applications","co":"2","sessions":1},{"srNo":3,"content":"Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation","co":"2","sessions":1},{"srNo":4,"content":"Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders, Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc, Gain deeper insights into the capabilities and potential of Transformer technology","co":"2","sessions":1},{"srNo":5,"content":"LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text, Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs, Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs, Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases","co":"2","sessions":1}],"Learning Resources":{"textBooks":["Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play by David Foster, O'Reilly Media. Second edition: June 6, 2023 (ISBN: 978-1098134181)","Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs by Ben Auffarth, Packt Publishing, first edition: December 22, 2023 (ISBN-13: 978-1835083468)"],"referenceLinks":["Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017). [Research Paper]","Yenduri, Gokul, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Rutvij H. Jhaveri, Weizheng Wang, Athanasios V. Vasilakos, and ThippaReddy Gadekallu. \"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions.\" arXiv preprint arXiv:2305.10435 (2023). [Research Paper]","Liu, Yang, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. \"Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.\" arXiv preprint arXiv:2308.05374 (2023). [Research Paper]"]},"internalAssessmentData":{"components":{"component1731776591362":{"component":"Assignment","duration":"2 Weeks","weightage":"20%","evaluationWeek":"After Mid Semester","remarks":"Continuous Participation + Viva"}}},"copoMappingData":{"courseOutcomes":{"CO1":{"description":"Develop a deep understanding of generative AI principles, encompassing various generative models and their practical applications for diverse real-world scenarios.","bullets":[]}},"mappingData":{"CO1":{"PO1":"3","PO2":"3","PO3":"3","PO4":"2","PO5":"3","PO6":"3","PO7":"3","PO8":"3","PO9":"3","PO10":"0","PO11":"0","PO12":"0","PSO1":"0","PSO2":"0","PSO3":"0","PSO4":"0"}}},"actionsForWeakStudentsData":[{"id":"1","text":""}]},{"filename":"1733316103New.pdf","done":1,"Program":"CoreElective–6thSem,CSE","Session":"3sessionsperweek [Eachsessionisof55minutes]","course_code":"CSE3720","course_name":"GenerativeAIandLLMs","Module/Semester":"6th Semester","course_description":"This course aims to provide students with a comprehensive understanding of generative artificial intelligence, focusing on large language models. Students will learn how to design, train, and deploy generative AI systems, with a particular emphasis on Large Language Models. They will explore the principles, applications, and ethical considerations surrounding these technologies. The course describes in detail the transformer architecture that powers LLMs, how they’re trained, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","Course Syllabus":{"courseSyllabus":[{"srNo":1,"content":"Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.","co":"1","sessions":1},{"srNo":2,"content":"Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational AutoencodersText-to-Text generation, Image and text-based VAE applications.","co":"2","sessions":1},{"srNo":3,"content":"Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.","co":"2","sessions":1},{"srNo":4,"content":"Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders. Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc. Gain deeper insights into the capabilities and potential of Transformer technology.","co":"2","sessions":1},{"srNo":5,"content":"LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text. Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs. Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs. Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","co":"2","sessions":1},{"srNo":6,"content":"Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts. Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its component.","co":"2","sessions":1},{"srNo":7,"content":"Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and Other ethical concerns Addressing bias and fairness in generative AI systems Ensuring responsible use and deployment of generative models.","co":"3","sessions":1}]},"Learning Resources":{"textBooks":["Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play by David Foster, O'Reilly Media. Second edition: June 6, 2023 (ISBN: 978-1098134181)","Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs by Ben Auffarth, Packt Publishing, first edition: December 22, 2023 (ISBN-13: 978-1835083468)"],"referenceLinks":["Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017). [Research Paper]","Yenduri, Gokul, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Rutvij H. Jhaveri, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy Gadekallu. \"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions.\" arXiv preprint arXiv:2305.10435 (2023). [Research Paper]","Liu, Yang, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. \"Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.\" arXiv preprint arXiv:2308.05374 (2023). [Research Paper]"]},"internalAssessmentData":{"components":{"component1731776591362":{"component":"Assignment","duration":"2 Weeks","weightage":"20%","evaluationWeek":"After Mid Semester","remarks":"Continuous Participation"}}},"copoMappingData":{"courseOutcomes":{"CO1":{"description":"Develop a deep understanding of generative AI principles, encompassing various generative models and their practical applications for diverse real-world scenarios.","bullets":[]}},"mappingData":{"CO1":{"PO1":"3","PO2":"3","PO3":"3","PO4":"2","PO5":"3","PO6":"3","PO7":"3","PO8":"3","PO9":"0","PO10":"0","PO11":"0","PO12":"0","PSO1":"0","PSO2":"0","PSO3":"0","PSO4":"0"}}},"actionsForWeakStudentsData":[{"id":"1","text":""}]},{"filename":"1733316317New.pdf","done":1,"Program":"CoreElective-6thSem, CSE","Session":"3sessionsperweek [Eachsessionisof55minutes]","course_code":"CSE3720","course_name":"Generative AI and LLMs","Module/Semester":"6th Semester","course_description":"This course aims to provide students with a comprehensive understanding of generative artificial intelligence, focusing on large language models. Students will learn how to design, train, and deploy generative AI systems, with a particular emphasis on Large Language Models. They will explore the principles, applications, and ethical considerations surrounding these technologies. The course describes in detail the transformer architecture that powers LLMs, how they’re trained, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","Course Syllabus":{"courseSyllabus":[{"srNo":1,"content":"Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.","co":"1","sessions":1},{"srNo":2,"content":"Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational Autoencoders Text-to-Text generation, Image and text-based VAE applications.","co":"2","sessions":1},{"srNo":3,"content":"Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.","co":"2","sessions":1},{"srNo":4,"content":"Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders. Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc. Gain deeper insights into the capabilities and potential of Transformer technology.","co":"2","sessions":1},{"srNo":5,"content":"LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text. Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs. Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs. Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","co":"2","sessions":1},{"srNo":6,"content":"Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts. Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its components.","co":"2","sessions":1},{"srNo":7,"content":"Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and Other ethical concerns Addressing bias and fairness in generative AI systems. Ensuring responsible use and deployment of generative models.","co":"3","sessions":1}]},"Learning Resources":{"textBooks":["Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play by David Foster, O'Reilly Media. Second edition: June 6, 2023 (ISBN: 978-1098134181)","Generative AI with LangChain: Build largelanguagemodel (LLM) apps with Python, ChatGPT, and other LLMs by Ben Auffarth, Packt Publishing, first edition: December 22, 2023 (ISBN-13: 978-1835083468)"],"referenceLinks":["Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 'Attention is all you need.' Advances in neural information processing systems 30 (2017). [Research Paper]","Yenduri, Gokul, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Rutvij H. Jhaveri, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy Gadekallu. 'Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions.' arXiv preprint arXiv:2305.10435 (2023). [Research Paper]","Liu, Yang, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 'Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment.' arXiv preprint arXiv:2308.05374 (2023). [Research Paper]"]},"internalAssessmentData":{"components":{"component1731776591362":{"component":"Assignment","duration":"2 Weeks","weightage":"20%","evaluationWeek":"AfterMid Assess","remarks":"Continuous Participation + Viva"}}},"copoMappingData":{"courseOutcomes":{"CO1":{"description":"Develop a deep understanding of generative AI principles, encompassing various generative models and their practical applications for diverse real-world scenarios.","bullets":[]}},"mappingData":{"CO1":{"PO1":"3","PO2":"3","PO3":"3","PO4":"2","PO5":"3","PO6":"3","PO7":"3","PO8":"3","PO9":"0","PO10":"0","PO11":"0","PO12":"0","PSO1":"0","PSO2":"0","PSO3":"0","PSO4":"0"}}},"actionsForWeakStudentsData":[{"id":"1","text":""}],"course_syllabus":{"courseSyllabus":[{"srNo":1,"content":"Generative AI: What is generative AI, Different types of generative AI models, Understand the principles and applications of Generative AI in creating new data instances, Understand the principles behind Generative AI: Get familiar with building and tweaking generative models for any real-world use case. Generative AI Project Lifecycle.","co":"1","sessions":1},{"srNo":2,"content":"Seq2Seq Models and Variational Autoencoders (VAEs): Sequence-to-Sequence models, Variational Autoencoders Text-to-Text generation, Image and text-based VAE applications.","co":"2","sessions":1},{"srNo":3,"content":"Generative Adversarial Networks (GANs): Understanding GAN architecture, GAN training process, GAN applications in image and video generation.","co":"2","sessions":1},{"srNo":4,"content":"Transformers: Learn about the architecture of Transformer models, including attention mechanisms, encoders, and decoders. Understand pre-training and fine-tuning strategies. Dive into popular Transformer models: BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) etc. Gain deeper insights into the capabilities and potential of Transformer technology.","co":"2","sessions":1},{"srNo":5,"content":"LLMs: Learn about Language Models (LLMs) and their role in understanding and generating human-like text. Different types of LLMs, Different training datasets for LLMs, Training methods for LLMs. Challenges in training LLMs, Different evaluation metrics for LLMs, Challenges in evaluating LLMs. Practical Implementation of LLMs, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases.","co":"2","sessions":1},{"srNo":6,"content":"Prompts Module: Understanding the concept and significance of prompt engineering, strategies for designing effective prompts. Techniques for controlling model behavior and output quality, Introduction to LangChain and its objectives, Overview of the LangChain framework and its components.","co":"2","sessions":1},{"srNo":7,"content":"Ethical Considerations of Generative AI and LLMs: Potential biases in generative AI models, Misinformation and disinformation generated by LLMs and Other ethical concerns Addressing bias and fairness in generative AI systems. Ensuring responsible use and deployment of generative models.","co":"3","sessions":1}]},"studentListData":[{"uniqueId":"210C2030002","studentName":"Aayush Dubey"},{"uniqueId":"210C2030004","studentName":"PelletiSujith Reddy"},{"uniqueId":"210C2030007","studentName":"Malladi Sai Prabhas"},{"uniqueId":"210C2030010","studentName":"SubhranshBehura"},{"uniqueId":"210C2030014","studentName":"Abhimanyu Gulati"}],"weakStudentsData":[{"uniqueId":"210C2030002","studentName":"Aayush Dubey","totalMarks":83,"grade":"A+"},{"uniqueId":"210C2030004","studentName":"PelletiSujith Reddy","totalMarks":78.5,"grade":"B"},{"uniqueId":"210C2030007","studentName":"Malladi Sai Prabhas","totalMarks":81.5,"grade":"B+"},{"uniqueId":"210C2030010","studentName":"SubhranshBehura","totalMarks":87,"grade":"A+"},{"uniqueId":"210C2030014","studentName":"Abhimanyu Gulati","totalMarks":47,"grade":"R"}],"marksDetailsData":[{"uniqueId":"210C2030002","studentName":"Aayush Dubey","totalMarks":83,"grade":"A+"},{"uniqueId":"210C2030004","studentName":"PelletiSujith Reddy","totalMarks":78.5,"grade":"B"},{"uniqueId":"210C2030007","studentName":"Malladi Sai Prabhas","totalMarks":81.5,"grade":"B+"},{"uniqueId":"210C2030010","studentName":"SubhranshBehura","totalMarks":87,"grade":"A+"},{"uniqueId":"210C2030014","studentName":"Abhimanyu Gulati","totalMarks":47,"grade":"R"}],"attendanceReportData":[{"uniqueId":"210C2030002","studentName":"Aayush Dubey","attendance":88},{"uniqueId":"210C2030004","studentName":"PelletiSujith Reddy","attendance":90},{"uniqueId":"210C2030007","studentName":"Malladi Sai Prabhas","attendance":78},{"uniqueId":"210C2030010","studentName":"SubhranshBehura","attendance":82},{"uniqueId":"210C2030014","studentName":"Abhimanyu Gulati","attendance":68}]}]